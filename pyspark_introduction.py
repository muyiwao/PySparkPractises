# -*- coding: utf-8 -*-
"""Pyspark_Introduction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kqtikL9PTLD0PYqdRqP_CjVe4QZLsoSu
"""

pip install pyspark

pip install pyspark

from pyspark import SparkConf

from pyspark.context import SparkContext
conf = SparkConf().setAppName('Pyspark').setMaster('local')
sc = SparkContext(conf=conf)

"""sample"""

values = [1,2,3,4,5]
rdd = sc.parallelize(values)

rdd.take(5)

from google.colab import files
uploaded = files.upload()

rdd = sc.textFile("test.txt")

rdd.take(1)

rdd.collect()



aba = sc.parallelize(range(1,10000,2))
aba.persist()

textfile = sc.textFile("test.txt")
textfile.cache()

"""Map"""

x = sc.parallelize(["spark", "rdd", "example", "sample", "example"])
y = x.map(lambda x:(x,1))
y.collect()

"""FlatMap"""

rdd = sc.parallelize([2,3,4])
sorted(rdd.flatMap(lambda x: range(1,x)).collect())

"""Filter"""

rdd = sc.parallelize([1,2,3,4,5])
rdd.filter(lambda x: x%2 == 0).collect()

"""Sample"""

parrallel = sc.parallelize(range(9))
parrallel.sample(True, .2).count()
parrallel.sample(False,1).collect()

"""union"""

parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.union(par).collect()

"""intersection"""

parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.intersection(par).collect()

"""distinct"""

parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.union(par).distinct().collect()

"""sortby"""

y = sc.parallelize([5,7,1,3,2,1])
y.sortBy(lambda c: c, True).collect()

"""sortby"""

z = sc.parallelize([("H", 10), ("A", 26),("F", 3) ,("Z", 1), ("L", 5)])
z.sortBy(lambda c: c, False).collect()

"""MapPartitions"""

rdd = sc.parallelize([1,2,3,4], 2)
def f(iterator): yield sum(iterator)
rdd.mapPartitions(f).collect()

"""MapPartitions - Withindex"""

rdd = sc.parallelize([1,2,3,4], 4)
def f(splitIndex, Iterator): yield splitIndex
rdd.mapPartitionsWithIndex(f).sum()

"""GroupBy"""

rdd = sc.parallelize([1,1,2,3,5,8])
result = rdd.groupBy(lambda x: x%2).collect()
sorted ([(x, sorted(y)) for (x,y) in result])

"""keyBy"""

x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)
y = sc.parallelize(zip(range(0,5), range(0,5)))
[(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]

"""Zip"""

x = sc.parallelize(range(0,5))
y = sc.parallelize(range(1000, 1005))
x.zip(y).collect()

"""Zip - Withinindex"""

sc.parallelize(["a", "b", "c", "d"], 3).zipWithIndex().collect()